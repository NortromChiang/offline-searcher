1. site
site information displayed in the right top area of stackdump web page.
site information will be fetched when indexing(app.py site_index() and index())
site contains:
{'sites': [<Site 2 name=u'datascience' desc=u'data science' key=u'ds' dump_date=u'August 2017' import_date='datetime.datetime...)' base_url="u'datascience.sta...'">, <Site 1 name=u'manpages' desc=u'linux manpages' key=u'man' dump_date=u'August 2017' import_date='datetime.datetime...)' base_url=u'linux.manpage'>]}
{'sites': [<Site 2 name=u'datascience' desc=u'data science' key=u'ds' dump_date=u'August 2017' import_date='datetime.datetime...)' base_url="u'datascience.sta...'">, <Site 1 name=u'manpages' desc=u'linux manpages' key=u'man' dump_date=u'August 2017' import_date='datetime.datetime...)' base_url=u'linux.manpage'>], 'site': <Site 2 name=u'datascience' desc=u'data science' key=u'ds' dump_date=u'August 2017' import_date='datetime.datetime...)' base_url="u'datascience.sta...'">}

2. result
result information displayed in the middle area of stackdump web page.
result information are solr returned(view_question() in app.py)
	it has the following k-v pair:
siteKey
ownerUserId
_version_
id
documentId
question-json
	body viewCount title lastEditorUserId lastActivityDate comments [] score ownerUserId creationDate favoriteCount id tags
answers-json
	body title lastActivityDate comments commentCount score parentId ownerUserId creationDate favoriteCount

example:
decode_json_fields element in result
{u'siteKey': u'ds', u'question-json': u'{"body": "<p>I have a list of about 20 tasks that a machine need to be performed by a machine. Each tasks consists of a sequence of 3 to 5 actions that must be executed sequentially to complete a task. There are a total of 50 different actions.</p>\\n\\n<p>I want an algorithm to learn which actions to execute to complete the tasks. To learn this I have a dataset of approximately 1000 samples. Each sample consists of a task with corresponding actions. </p>\\n\\n<p>Given a task $T_n$, the machine needs to come up with the sequence of actions to complete the task.</p>\\n\\n<p>Sample dataset ($T$ for tasks out of 20, $A$ for actions out of 50):<br>\\n$T_1$ - $(A_5 \\\\rightarrow A_1 \\\\rightarrow A_3)$<br>\\n$T_2$ - $(A_7\\\\rightarrow A_12\\\\rightarrow A_4\\\\rightarrow A_1)$<br>\\n$T_1$ - $(A_5\\\\rightarrow A_3\\\\rightarrow A_{13})$  </p>\\n\\n<p>What is the best way to solve this?</p>\\n\\n<p>Based on my knowledge of linear regression and classification, I believe that both are not the right approach to solve the task at hand.</p>\\n\\n<h3>EDIT 1:</h3>\\n\\n<p>One way I can think to solve this problem.\\nThe machine doesn\'t have to come up with the whole sequence instantly.</p>\\n\\n<p>If a new task has to be done, the machine has an option to choose the next action (first action) from, say, 5 available actions it can perform. After performing this action, it would have, say, 5 more actions and it has to choose it\'s next action to complete. And go on doing these actions to complete the given task.</p>\\n\\n<p>Is this kind of a finite state machine?</p>\\n", "viewCount": 51, "title": "Identifying sequences of actions required to complete tasks, based on data of completed tasks", "lastEditorUserId": 836, "lastActivityDate": "2017-02-16T08:13:01.680Z", "comments": [{"text": "Welcome to DataScience SE! Could you clarify your question? I don\'t see how a question about tasks and actions are related to data science.", "creationDate": "2017-02-16T07:53:33", "score": 0, "userId": 12384, "id": 11794}, {"text": "Thanks. I think the machine needs to complete tasks by learning from the dataset of samples on how to solve tasks by sequentially selecting actions. That is a machine learning problem. So, I asked here as the description suggests ML problems are welcome.", "creationDate": "2017-02-16T07:58:37", "score": 0, "userId": 3299, "id": 11795}, {"text": "I would store the data in a graph data base such as [Neo4J](https://neo4j.com/) and store the tasks and actions as nodes. You will quickly be able to detect the most frequently used path in the graph. It is not very clear from your description why a same task can have different actions, maybe you could clarify this?", "creationDate": "2017-02-16T08:09:12", "score": 1, "userId": 12384, "id": 11796}, {"text": "Could you clarify how strictly the tasks are defined, and how the data relates? Is it possible for instance to infer that a task $T_n$ can be completed using a *minimal* set of ordered actions, based on all instances of $T_n$ in the database. Or can tasks be completed in more than one way - so for example $T_1$ might have actions $(A_5\\u2192A_1\\u2192A_3)$ and $(A_7\\u2192A_4\\u2192A_9)$, there are at least two valid but entirely separate ways to get it done?", "creationDate": "2017-02-16T08:16:55", "score": 0, "userId": 836, "id": 11797}, {"text": "If this was not phrased as a supervised learning problem, and there was a notion of current *state* (perhaps even defined by the available actions), then you could look into [Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning). If you have some model of state you could potentially use simple [Q-learning](https://en.wikipedia.org/wiki/Q-learning) from the existing data, with a reward of -1 per action.", "creationDate": "2017-02-16T08:22:27", "score": 0, "userId": 836, "id": 11798}, {"text": "@Stereo I am not planning to tell the problem I am solving, but I\'ll try my best to answer your question. So, a task can only be completed in one way in a given environment (available actions after completing previous action). We have to choose the best possible action from the available 5 or so actions at each step. There are situations where we can\'t choose A1 in second step while doing T1 as it is not available in this environment.", "creationDate": "2017-02-16T09:39:58", "score": 0, "userId": 3299, "id": 11799}, {"text": "@NeilSlater So, in any given environment there is only one way to do the task. But if we want to perform the same task in the new environment, we have to change the sequence or the actions. Thanks for your RL solution. I would look into that.", "creationDate": "2017-02-16T09:44:24", "score": 0, "userId": 3299, "id": 11800}, {"text": "I would recommend to add your comment to your original question, also try to write your tasks as $T_1$ instead of T1. It makes the reading a bit easier.", "creationDate": "2017-02-16T12:52:03", "score": 0, "userId": 12384, "id": 11801}, {"text": "@Stereo Yeah, I would edit it.", "creationDate": "2017-02-16T13:36:04", "score": 0, "userId": 3299, "id": 11802}], "score": 4, "ownerUserId": 3299, "creationDate": "2017-02-16T07:32:17.380Z", "favoriteCount": 0, "id": 17012, "tags": ["machine-learning", "supervised-learning"]}', u'ownerUserId': 3299, u'_version_': 1579645190473252864L, u'id': u'17012', u'documentId': u'ds-17012'}
decode_json_fields element in result


{u'siteKey': u'ds', u'question-json': u'{"body": "<p>Is there a known general table of statistical techniques that explain how they scale with sample size and dimension? For example, a friend of mine told me the other day that the computation time of simply quick-sorting one dimensional data of size n goes as n*log(n).</p>\\n\\n<p>So, for example, if we regress y against X where X is a d-dimensional variable, does it go as O(n^2*d)? How does it scale if I want to find the solution via exact Gauss-Markov solution vs numerical least squares with Newton method? Or simply getting the solution vs using significance tests?</p>\\n\\n<p>I guess I more want a good source of answers (like a paper that summarizes the scaling of various statistical techniques) than a good answer here. Like, say, a list that includes the scaling of multiple regression, logistic regression, PCA, cox proportional hazard regression, K-means clustering, etc.</p>\\n", "viewCount": 202, "title": "How do various statistical techniques (regression, PCA, etc) scale with sample size and dimension?", "lastEditorUserId": 2841, "lastActivityDate": "2014-08-14T17:28:40.453Z", "comments": [{"text": "This is a good question. A lot of statistics books talk about the theoretical aspects of high-dimensional data and not the computational aspects.", "creationDate": "2014-08-06T00:23:33", "score": 0, "userId": 1156, "id": 808}, {"text": "In many cases, the original literature will discuss complexity. But often theoretical complexity is useless. QuickSort has a worst-case of O(n^2), but often is the fastest - faster than HeapSort, which has worst case O(n log n).\\nIf you do a little research, you will find out complexity results for many algorithms - if known. E.g. PCA being O(n d^3), k-means being O(n k i d) etc.", "creationDate": "2014-08-16T10:39:50", "score": 0, "userId": 924, "id": 872}], "score": 8, "ownerUserId": 2841, "creationDate": "2014-08-05T18:36:12.753Z", "favoriteCount": 1, "id": 915, "tags": ["bigdata", "statistics", "efficiency", "scalability"]}', u'answers-json': [u'{"body": "<p>Most of the efficient (and non trivial) statistic algorithms are iterative in nature so that the worst case analysis <code>O()</code> is irrelevant as the worst case is \'it fails to converge\'.</p>\\n\\n<p>Nevertheless, when you have a lot of data, even the linear algorithms (<code>O(n)</code>) can be slow and you then need to focus on the constant \'hidden\' behind the notation. For instance, computing the variance of a single variate is naively done scanning the data twice (once for computing an estimate of the mean, and then once to estimate the variance). But it also can be done in <a href=\\"http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\\">one pass</a>.</p>\\n\\n<p>For iterative algorithms, what is more important is convergence rate and number of parameters as a function of the data dimensionality, an element that greatly influences convergence. Many models/algorithm grow a number of parameters that is exponential with the number of variables (e.g. splines) while some other grow linearly (e.g. support vector machines, random forests, ...) </p>\\n", "title": "", "lastActivityDate": "2014-08-05T20:24:09.200Z", "comments": [{"text": "I\'m not sure I agree with this: when designing an algorithm for a statistical problem, a *lot* of concern goes into the complexity of each iterative step (and is usually documented in a manuscript). But as you point out, often it\'s not that easy to summarize, as two algorithms with the same complexity per iteration may perform very differently due to necessary iterations. That being said, it\'s very rare that the number of iterations required grows faster than `O(log(n) )`.", "creationDate": "2015-09-26T02:12:27", "score": 0, "userId": 13005, "id": 3816}], "commentCount": 1, "score": 5, "parentId": 915, "ownerUserId": 172, "creationDate": "2014-08-05T20:24:09.200Z", "favoriteCount": 0, "id": 916}', u'{"body": "<p>You mentioned regression and PCA in the title, and there is a definite answer for each of those.</p>\\n\\n<p>The asymptotic complexity of linear regression reduces to O(P^2 * N) if N > P, where P is the number of features and N is the number of observations. More detail in <a href=\\"https://math.stackexchange.com/a/84503/117452\\">Computational complexity of least square regression operation</a>.</p>\\n\\n<p>Vanilla PCA is O(P^2 * N + P ^ 3), as in <a href=\\"https://scicomp.stackexchange.com/q/3220\\">Fastest PCA algorithm for high-dimensional data</a>. However fast algorithms exist for very large matrices, explained in that answer and <a href=\\"https://stats.stackexchange.com/q/2806/36229\\">Best PCA Algorithm For Huge Number of Features?</a>.</p>\\n\\n<p>However I don\'t think anyone\'s compiled a single lit review or reference or  book on the subject. Might not be a bad project for my free time...</p>\\n", "title": "", "lastEditorUserId": -1, "lastActivityDate": "2014-08-08T00:25:04.210Z", "lastEditDate": "2017-04-13T12:53:55.957Z", "comments": [{"text": "Thanks, that\'s very helpful! If you do a literature review for various predictive modelling techniques, I\'m sure it would get referenced a lot. It would be very helpful for people who want to differentiate between which algorithms to use in large n or large p cases, or for medium values of those for more precise calculations. Do you happen to know how some of the more obscure techniques scale? (Like Cox proportional hazard regression or confirmatory factor analysis)", "creationDate": "2014-08-08T13:31:01", "score": 0, "userId": 2841, "id": 839}, {"text": "Unfortunately no, but if I ever do that review I will try to be comprehensive. I\'d hardly call Cox regression \\"obscure,\\" at least in my field.", "creationDate": "2014-08-19T16:31:00", "score": 0, "userId": 1156, "id": 899}], "commentCount": 2, "score": 4, "parentId": 915, "ownerUserId": 1156, "creationDate": "2014-08-08T00:25:04.210Z", "favoriteCount": 0, "id": 942}', u'{"body": "<p>I gave a very limited partial answer for the confirmatory factor analysis package that I developed for Stata in this <a href=\\"http://www.stata-journal.com/article.html?article=st0169\\" rel=\\"nofollow\\">Stata Journal article</a> based on timing the actual simulations. Confirmatory factor analysis was implemented as a maximum likelihood estimation technique, and I could see very easily how the computation time grew with each dimension (sample size <code>n</code>, number of variables <code>p</code>, number of factors <code>k</code>). As it is heavily dependent on how Stata thinks about the data (optimized to compute across columns/observations rather than rows), I found performance to be <code>O(n^{0.68} (k+p)^{2.4})</code> where 2.4 is the fastest matrix inversion asymptotics (and there\'s hell of a lot of that in confirmatory factor analysis iterative maximization). I did not give a reference for the latter, but I think I got this from <a href=\\"http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra\\" rel=\\"nofollow\\">Wikipedia</a>.</p>\\n\\n<p>Note that there is also a matrix inversion step in OLS. However, for reasons of numerical accuracy, no one would really brute-force inverse the <code>X\'X</code> matrix, and would rather use sweep operators and identify the dangerously collinear variables to deal with precision issues. If you add up $10^8$ numbers that originally were in <a href=\\"http://en.wikipedia.org/wiki/Double-precision_floating-point_format\\" rel=\\"nofollow\\">double precision</a>, you will likely end up with a number that only has a single precision. Numerical computing issues may become a forgotten corner of big data calculations as you start optimizing for speed.</p>\\n", "title": "", "lastActivityDate": "2014-08-14T17:28:40.453Z", "comments": [{"text": "Math formatting does not work on DataScience? Really? May be we should ask to get it.", "creationDate": "2014-08-14T17:29:51", "score": 2, "userId": 1237, "id": 867}, {"text": "Good point about numerical accuracy.", "creationDate": "2014-08-15T00:24:26", "score": 0, "userId": 1156, "id": 868}], "commentCount": 2, "score": 4, "parentId": 915, "ownerUserId": 1237, "creationDate": "2014-08-14T17:28:40.453Z", "favoriteCount": 0, "id": 976}'], u'ownerUserId': 2841, u'_version_': 1579645118450761728L, u'id': u'915', u'documentId': u'ds-915'}
siteKey
question-json
answers-json
in json value

{"body": "<p>Most of the efficient (and non trivial) statistic algorithms are iterative in nature so that the worst case analysis <code>O()</code> is irrelevant as the worst case is 'it fails to converge'.</p>\n\n<p>Nevertheless, when you have a lot of data, even the linear algorithms (<code>O(n)</code>) can be slow and you then need to focus on the constant 'hidden' behind the notation. For instance, computing the variance of a single variate is naively done scanning the data twice (once for computing an estimate of the mean, and then once to estimate the variance). But it also can be done in <a href=\"http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\">one pass</a>.</p>\n\n<p>For iterative algorithms, what is more important is convergence rate and number of parameters as a function of the data dimensionality, an element that greatly influences convergence. Many models/algorithm grow a number of parameters that is exponential with the number of variables (e.g. splines) while some other grow linearly (e.g. support vector machines, random forests, ...) </p>\n", "title": "", "lastActivityDate": "2014-08-05T20:24:09.200Z", "comments": [{"text": "I'm not sure I agree with this: when designing an algorithm for a statistical problem, a *lot* of concern goes into the complexity of each iterative step (and is usually documented in a manuscript). But as you point out, often it's not that easy to summarize, as two algorithms with the same complexity per iteration may perform very differently due to necessary iterations. That being said, it's very rare that the number of iterations required grows faster than `O(log(n) )`.", "creationDate": "2015-09-26T02:12:27", "score": 0, "userId": 13005, "id": 3816}], "commentCount": 1, "score": 5, "parentId": 915, "ownerUserId": 172, "creationDate": "2014-08-05T20:24:09.200Z", "favoriteCount": 0, "id": 916}
{"body": "<p>You mentioned regression and PCA in the title, and there is a definite answer for each of those.</p>\n\n<p>The asymptotic complexity of linear regression reduces to O(P^2 * N) if N > P, where P is the number of features and N is the number of observations. More detail in <a href=\"https://math.stackexchange.com/a/84503/117452\">Computational complexity of least square regression operation</a>.</p>\n\n<p>Vanilla PCA is O(P^2 * N + P ^ 3), as in <a href=\"https://scicomp.stackexchange.com/q/3220\">Fastest PCA algorithm for high-dimensional data</a>. However fast algorithms exist for very large matrices, explained in that answer and <a href=\"https://stats.stackexchange.com/q/2806/36229\">Best PCA Algorithm For Huge Number of Features?</a>.</p>\n\n<p>However I don't think anyone's compiled a single lit review or reference or  book on the subject. Might not be a bad project for my free time...</p>\n", "title": "", "lastEditorUserId": -1, "lastActivityDate": "2014-08-08T00:25:04.210Z", "lastEditDate": "2017-04-13T12:53:55.957Z", "comments": [{"text": "Thanks, that's very helpful! If you do a literature review for various predictive modelling techniques, I'm sure it would get referenced a lot. It would be very helpful for people who want to differentiate between which algorithms to use in large n or large p cases, or for medium values of those for more precise calculations. Do you happen to know how some of the more obscure techniques scale? (Like Cox proportional hazard regression or confirmatory factor analysis)", "creationDate": "2014-08-08T13:31:01", "score": 0, "userId": 2841, "id": 839}, {"text": "Unfortunately no, but if I ever do that review I will try to be comprehensive. I'd hardly call Cox regression \"obscure,\" at least in my field.", "creationDate": "2014-08-19T16:31:00", "score": 0, "userId": 1156, "id": 899}], "commentCount": 2, "score": 4, "parentId": 915, "ownerUserId": 1156, "creationDate": "2014-08-08T00:25:04.210Z", "favoriteCount": 0, "id": 942}
{"body": "<p>I gave a very limited partial answer for the confirmatory factor analysis package that I developed for Stata in this <a href=\"http://www.stata-journal.com/article.html?article=st0169\" rel=\"nofollow\">Stata Journal article</a> based on timing the actual simulations. Confirmatory factor analysis was implemented as a maximum likelihood estimation technique, and I could see very easily how the computation time grew with each dimension (sample size <code>n</code>, number of variables <code>p</code>, number of factors <code>k</code>). As it is heavily dependent on how Stata thinks about the data (optimized to compute across columns/observations rather than rows), I found performance to be <code>O(n^{0.68} (k+p)^{2.4})</code> where 2.4 is the fastest matrix inversion asymptotics (and there's hell of a lot of that in confirmatory factor analysis iterative maximization). I did not give a reference for the latter, but I think I got this from <a href=\"http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra\" rel=\"nofollow\">Wikipedia</a>.</p>\n\n<p>Note that there is also a matrix inversion step in OLS. However, for reasons of numerical accuracy, no one would really brute-force inverse the <code>X'X</code> matrix, and would rather use sweep operators and identify the dangerously collinear variables to deal with precision issues. If you add up $10^8$ numbers that originally were in <a href=\"http://en.wikipedia.org/wiki/Double-precision_floating-point_format\" rel=\"nofollow\">double precision</a>, you will likely end up with a number that only has a single precision. Numerical computing issues may become a forgotten corner of big data calculations as you start optimizing for speed.</p>\n", "title": "", "lastActivityDate": "2014-08-14T17:28:40.453Z", "comments": [{"text": "Math formatting does not work on DataScience? Really? May be we should ask to get it.", "creationDate": "2014-08-14T17:29:51", "score": 2, "userId": 1237, "id": 867}, {"text": "Good point about numerical accuracy.", "creationDate": "2014-08-15T00:24:26", "score": 0, "userId": 1156, "id": 868}], "commentCount": 2, "score": 4, "parentId": 915, "ownerUserId": 1237, "creationDate": "2014-08-14T17:28:40.453Z", "favoriteCount": 0, "id": 976}
ownerUserId
_version_
id
documentId
decode_json_fields result:
<pysolr.Results object at 0xb3e545ec>
return context
{'rows_per_page': 10, 'current_page': 1, 'total_pages': 1, 'results': <pysolr.Results object at 0xb3e545ec>, 'sort_by': 'relevance', 'total_hits': 4, 'query': 'algorithm here in datascience'}


3. solr
pysolr sending the following field to solr, which solr can indexing
in : solr/stackdump/conf/schema.xml

4. others
cp ~/vmshare/work/import_site.py ~/vmshare/stackdump/python/src/stackdump/commands/
start server
	./start_solr.sh
start searching
	./start_web.sh
import site manpage
	./manage.sh import_site --mode manpages --base-url "linux.manpage" --dump-date "August 2017" --site-name "manpages" --site-key "man" --site-desc "linux manpages" "/home/nortrom/vmshare/work/manpages/linux/man-pages/man7"
import stackoverflow
	./manage.sh import_site --mode stackoverflow --base-url datascience.stackexchange.com --dump-date "August 2017" ~/vmshare/datascience.stackexchange.com/ --site-name "datascience" --site-key "ds" --site-desc "data science"

view answers in solr
http://localhost:8983/solr/#/stackdump/query

view answers in stackdump
http://0.0.0.0:8080/
